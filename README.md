# Assignment Info
COSI132A Information Retrieval Spring 2022 - Final Project: Analysis on Different Ways of Applying NLP on Search Engine
(Orginal Code based on PA5)

## Description
* This is documentation of the final project for course COSI 132A Information Retrieval. 
* A TREC 2018 core corpus subset and twelve TREC topics with relevance judgments will be used for system development and result evaluation.
* In this project, we will provide example code for:
  - Populating and querying a corpus using ES
  - Implementing NDCG (normalized discounted cumulative gain) evaluation metric
  - Experimenting with “semantic” indexing and searching using simSCE paragraph embedding and topic modeling embedding
* The system consists of several key components: 
  - Displaying the similarity score between the search query and each of the documents
  - Indexing the corpus into ES with default standard analyzer and English analyzer for the text fields
  - Integrate ES into the Flask service for interactive search. Beside the traditional lexical search, the system should also allow the user to select the text representation to use for searching.
  - Evaluate the performance of 12 provided TREC query pairs using NDCG. For each of the 12 query pairs, produce a result table along with a brief analysis.


## Team Member Contribution
The team members include Capo Wang, Xiya Guan, Wanyue Xiao. Each of the team member equally contributed to this project.

**Capo Wang**: Unsupervised- and unsupervised simSCE Embedding with corresponding result analysis <br>
**Xiya Guan**: Topic Modeling Feature Embedding with corresponding result analysis <br>
**Wanyue Xiao**: Front End Web Development, "Did you mean" feature, Sorting Option, Time Filtering Option <br>


## Dataset
A larger subset of TREC 2018 core corpus that has already been processed. Specifically, each document has the following fields:

| variable      | Description                                                      |
| ------------- | ---------------------------------------------------------------- |
| doc_id        | original document id from the jsonline file                      |
| title         | article title                                                    |
| author        | article authors                                                  |
| content       | main article content (HTML tags removed)                         |
| date          | publish date in the format “yyyy/MM/dd”                          |
| annotation    | annotation for its relevance to a topic                          |
| ft_vector     | fastText embedding of the content                                |
| sbert_vector  | Sentence BERT embedding of the content                           |
| simSCE        | unsupervised simSCE embedding of the document                     |
| topic_feature | topic embeddings generated by LDA                                |
| sup_simCSE_vector | supervised simSCE embedding of the document                               |
| sup_simCSE_para_max |  supervised simSCE embedding by viewing three sentences as a paragraph via max pooling  |
| sup_simCSE_para_mean |supervised simSCE embedding by viewing three sentences as a paragraph via mean pooling   |


**Notes**:
* For the annotation field, the value is stored as the format of topic_id-relevance. The relevance can be either 0, 1 or 2, which represents irrelevant, relevant or very relevant.
* The topic id can be mapped to the query pairs in the file pa5_data/pa5_queries.json.
* If the annotation field is empty, it can be considered that this document is irrelevant to any topics.
* Only simCSE among 4 simCSE will be used for demo.

## Getting Started
### 1. Dependencies
This repository is Python-based, and **Python 3.9** is recommended. The dependencies include JSON, Flask, DateTime, re, elasticsearch, elasticsearch-dsl, sentence-transformers, flask, numpy, and pyzmq. 

### 2. First-time Running
All dependencies are listed in the requirement.txt file. Anyone who wishes to run this project on a local environment could install these packages using the command: <code>pip3 install -r requirements.txt </code>. Before running this project in the terminal, the user shall be aware that all the required packages listed above shall be properly installed or upgraded to the latest version. 

### 3. Download all Necessary Datasets
The data directory should contain the following files.
```
data
├── pa5_queries.json
├── ideal_relevance.json
├── all_embeddings_wapo.jl
├── topics2018.xml
└── wiki-news-300d-1M-subword.vec
```
You need to download the pretrained fastText embedding on wiki news and put it into data folder. You can click this [link](https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip) to download. 
If you want to use topic embeddings, you need to contact one of the author to get the pretrained topic model.

### 4. Activate Elasticsearch Basics
You can  can click this [link](https://www.elastic.co/downloads/past-releases#elasticsearch) to download ES. Make sure you are choosing Elasticsearch 7.10.2. 

To start the ES engine:
```shell script
cd elasticsearch-7.10.2/
./bin/elasticsearch
```
To test your ES is running, open http://localhost:9200/ in your browser. You should be able to see the health status of your ES instance with the version number, the name and more. **Note that you should keep ES running in the backend while you are building and using your IR system.**

### 5. Activate Embedding Service
Load fasttext embeddings that are trained on wiki news. Each embedding has 300 dimensions
```shell script
python -m embedding_service.server --embedding fasttext  --model pa5_data/wiki-news-300d-1M-subword.vec
```

Load sentence BERT embeddings that are trained on msmarco. Each embedding has 768 dimensions
```shell script
python -m embedding_service.server --embedding sbert  --model msmarco-distilbert-base-v3
```

Load topic embeddings that are trained on msmarco. Each embedding has 50 dimensions
```shell script
python -m embedding_service.server --embedding topic  --model model
```

Load simCSE Embedding
```shell script
python -m embedding_service.server --embedding simCSE  --model princeton-nlp/unsup-simcse-bert-base-uncased
```

Load sup_simCSE Embedding
```shell script
python -m embedding_service.server --embedding supsimCSE  --model princeton-nlp/sup-simcse-bert-base-uncased
```


if you want to use all embedding, run
```shell script
python load_es_index.py --index_name wapo_docs_50k --wapo_path pa5_data/all_embeddings_wapo.jl
```

**Note that you should keep all these shells running in the backend while you are building and using your IR system.**


### 6. Running the Programs
The user shall follow the following step to run this program in the local environment. Run <code> python hw5.py </code> in the environment and type http://127.0.0.1:5000/ in browser to view the web application. 

For Evaluation: 
Change ```TOPIC_ID``` to the topic ID you want to evaluate.
```shell
python evaluate.py --index_name wapo_docs_50k --use_english_analyzer --top_k 20

python evaluate_Capo.py --index_name wapo_docs_50k --use_english_analyzer --top_k 20
```

## Testing
###  TREC Topic for Evaluation: tunnel injury disaster
The evaluation for the key words of the topic #363 will be used for testing and demonstration.
```xml
<top>
<num> Number: 363 </num>
<title>
transportation tunnel disasters 
</title>
<desc> Description:
What disasters have occurred in tunnels used for transportation?  
</desc>
<narr> Narrative
A relevant document identifies a disaster in a tunnel used for trains, motor vehicles, or people. Wind tunnels and tunnels used for wiring, sewage, water, oil, etc. are not relevant. The cause of the problem may be fire, earthquake, flood, or explosion and can be accidental or planned. Documents that discuss tunnel disasters occurring during construction of a tunnel are relevant if lives were threatened.  
</narr>
</top>
```

### Output
In the following table, the evaluation scores for the example query with different combinations have been displayed. The number of the retrieved document was 20. 

| Evaluation | BM25+Standard  | BM25+English | SentenceBERT+English  | Rerank SentenceBERT+English  |
| ---------- | -------------- | ------------ | --------------------- | ---------------------------- |
| AP         | 0.84557        | 0.88460      | 0.72598               | 0.75250                      |
| Precision  | 0.85000        | 0.75000      | 0.70000               | 0.75000                      |
| NDCG@20    | 0.79794        | 0.76928      | 0.66862               | 0.63404                      |


### Output after using topic embeddings 
#### concatenating sbert/fasttext with topic embeddings
![concatenating sbert/fasttext with topic embeddings](https://github.com/xwanyue0221/COSI132A_Final_Project/blob/master/images/concate.png)
#### reranking with topic embeddings
![reranking with topic embeddings](https://github.com/xwanyue0221/COSI132A_Final_Project/blob/master/images/rerank.png)


###  Output after using simCSE embeddings

Embeddings Results 12 Average

|name| kw | nl |
|----|----|----|
|vector_bm25 | 0.3666 |0.2323 |
|rerank_ft|0.3328|0.2088|
|rerank_sbert|0.3615|0.2706|
|rerank_simCSE|0.3333|0.2464|
|rerank_supsimCSE|0.2991|0.2196|
|rerank_para_mean|0.3002|0.2202|
|rerank_para_max|0.3047|0.2156|
